ğŸ“˜ RAG Chatbot â€“ Hybrid Intent + Context-Aware Retrieval System

A Streamlit-based Retrieval-Augmented Generation (RAG) chatbot using hybrid intent routing, conversational memory, and contradiction-safe answering.

ğŸš€ Overview

This project is a complete Retrieval-Augmented Generation (RAG) chatbot built with:

Streamlit for UI

LangChain for embeddings + vector search

Groq LLM for answering

Custom Retrieval Agent with:

Repeated question detection

Memory-only fast recall

Score-based retrieval filtering

Query rewriting

Conversation Memory

Hybrid Intent Router powered by LLM (GREETING, GOODBYE, CHITCHAT, CONTROL_COMMAND â†’ handled without retrieval)

Contradiction-safe Answer Agent

PDF + Website ingestion pipeline

The assistant answers ONLY using the content from the uploaded PDF or provided URL.
It never hallucinates beyond the context and reports contradictions transparently.

âœ¨ Key Features
ğŸ”¹ 1. Hybrid Intent Classifier (LLM-based)

Routes non-RAG queries before hitting retrieval:

GREETING

GOODBYE

CHITCHAT

CONTROL_COMMAND

All factual questions go directly to the RAG pipeline.

ğŸ”¹ 2. Retrieval Agent with Smart Logic

Handles all retrieval cases:

Repeated question detection

Memory-only answers

Retrieval failure

End-session intent

Document-based chunk retrieval with score filtering

Automatic query rewriting for better retrieval

ğŸ”¹ 3. Answer Agent with Contradiction Detection

The LLM checks for:

Numerical contradictions

Opposing statements

Conflicting claims

And responds neutrally while showing transparent reasoning.

ğŸ”¹ 4. Conversation Memory

Stores:

User questions

Assistant responses

Embeddings for repeated-question detection

Context for memory-only answers

ğŸ”¹ 5. PDF & Webpage Ingestion

PDF text extraction

Web HTML extraction + sanitization

Chunking

Embedding + Vectorstore creation

Supports:

multiple pages

multiple document types

ğŸ“‚ Project Structure
ğŸ“¦ rag-chatbot
â”‚
â”œâ”€â”€ app.py                     # Streamlit main UI
â”œâ”€â”€ ingestion.py               # PDF/URL ingestion & vectorstore builder
â”œâ”€â”€ retrieval_agent.py         # Smart retrieval logic + message classification
â”œâ”€â”€ answer_agent.py            # Contradiction-safe answering logic
â”œâ”€â”€ conversation_memory.py     # Conversational memory store
â”œâ”€â”€ hf_llm.py                  # Groq LLM wrapper
â”œâ”€â”€ intent_classifier.py       # LLM-based hybrid intent router
â”‚
â”œâ”€â”€ requirements.txt           # Dependencies
â””â”€â”€ README.md                  # Project documentation

ğŸ› ï¸ Installation
1. Clone the repository
git clone https://github.com/jinxforhaters/rag_agent.git
cd rag-chatbot

2. Install dependencies
pip install -r requirements.txt

3. Set up environment variables

Create a .env file:

QDRANT_URL=https://some-qdrant-instance.cloud
QDRANT_API_KEY=xxxxxxxxxxxxxxxxxxxx
GROQ_API_KEY=xxxxxxxxxxxxxxxxxxxx

â–¶ï¸ Usage
Run Streamlit app:
streamlit run app.py

In the UI:

Upload a PDF or enter a website URL

Click Ingest Source

Ask questions based on the document

The bot will:

Detect intent

Retrieve relevant chunks

Check contradictions

Provide clean structured answers

Show exact sources used

ğŸ§  How It Works (Architecture Overview)
ğŸ”¹ 1. User message

â†“

ğŸ”¹ 2. Intent Classifier

Handles GREETING / GOODBYE / CHITCHAT / CONTROL_COMMAND
â†“

ğŸ”¹ 3. Retrieval Agent

Reformulates query

Searches vectorstore

Score-based filtering

Detects repetition

Returns top context chunks
â†“

ğŸ”¹ 4. Answer Agent

Checks contradictions

Generates structured answer

Appends sources
â†“

ğŸ”¹ 5. Memory Updated

Helps detect repeated questions & context recall
